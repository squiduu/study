{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Stand Alone DL_KAIST.ipynb","provenance":[{"file_id":"https://github.com/heartcored98/Standalone-DeepLearning/blob/master/Lec3/Lab4_write_pretty_DL_code.ipynb","timestamp":1629341284565}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"cells":[{"cell_type":"code","metadata":{"id":"uRVlZq75JUJn"},"source":["!pip install -q torch==1.0.0 torchvision\n","import torch\n","print(torch.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PD4cIKKvKFCC"},"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import argparse\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"29UainWPco7Y"},"source":["## Data Preparation"]},{"cell_type":"code","metadata":{"id":"Cu753dPPKGkV"},"source":["transform = transforms.Compose(\n","    [transforms.ToTensor(), # ToTensor() 로 보내면 RGB 범위: 0 ~ 1이 됨\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) # 평균: 0.5, 표준편차: 0.5 로 노멀라이제이션\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","trainset, valset = torch.utils.data.random_split(trainset, [40000, 10000])\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","\n","\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n","                                          shuffle=True, num_workers=2)\n","valloader = torch.utils.data.DataLoader(valset, batch_size=4, \n","                                        shuffle=False)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n","                                         shuffle=False, num_workers=2)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RxnfFJwBcsAv"},"source":["## Model Architecture"]},{"cell_type":"code","metadata":{"id":"_G6bZbbkMWWt"},"source":["class MLP(nn.Module):\n","    def __init__(self, in_dim, out_dim, hid_dim, n_layer, act):\n","        super(MLP, self).__init__()\n","        self.in_dim = in_dim # 다른 함수에서도 in_dim 사용할 수 있어서 self.XX 형태로 할당\n","        self.out_dim = out_dim\n","        self.hid_dim = hid_dim\n","        self.n_layer = n_layer\n","        self.act = act # activation function\n","        self.fc = nn.Linear(self.in_dim, self.hid_dim)\n","        self.linears = nn.ModuleList()\n","\n","        for i in range(self.n_layer - 1): # 인풋 레이어까지 포함한 n_layer인 경우\n","            self.linears.append(nn.Linear(self.hid_dim, self.hid_dim))\n","\n","        self.fc2 = nn.Linear(self.hid_dim, self.out_dim)\n","\n","        if self.act == 'relu':\n","            self.act = nn.ReLU()\n","    \n","    def forward(self, x):\n","        x = self.act(self.fc(x))\n","        for fc in self.linears:\n","            x = self.act(fc(x))\n","        x = self.fc2(x)\n","        return x\n","\n","net = MLP(3072, 10, 100, 4, 'relu')\n","print(net)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gz3qJl6Pir6Z"},"source":["## Define a Loss function and Optimizer"]},{"cell_type":"code","metadata":{"id":"cvAkThXViihw"},"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=1e-3, momentum=0.9)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uX4VifhkjQwk"},"source":["## Train the Network"]},{"cell_type":"code","metadata":{"id":"CSJaY06Xja_n"},"source":["for epoch in range(2):\n","    runnint_loss = 0.0\n","\n","    for i, data in enumerate(trainloader, 0):\n","        inputs, labels = data\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        runnung_loss += loss.item()\n","        if i % 2000 == 1999:\n","            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000)\n","            running_loss = 0.0\n","\n","print('Finished Training')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"itGsp6jDWs_a"},"source":["## Define Experiment"]},{"cell_type":"code","metadata":{"id":"LiOCP6TqWw2V"},"source":["def experiment(args):\n","  \n","    net = MLP(args.in_dim, args.out_dim, args.hid_dim, args.n_layer, args.act)\n","    net.cuda()\n","    print(net)\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=args.mm)\n","    \n","    for epoch in range(args.epoch):  # loop over the dataset multiple times\n","\n","        # ==== Train ===== #\n","        net.train()\n","        \n","        running_loss = 0.0\n","        train_loss = 0.0\n","        for i, data in enumerate(trainloader, 0):\n","            optimizer.zero_grad() # [21.01.05 오류 수정] 매 Epoch 마다 .zero_grad()가 실행되는 것을 매 iteration 마다 실행되도록 수정했습니다. \n","\n","            # get the inputs\n","            inputs, labels = data\n","            inputs = inputs.view(-1, 3072)\n","            \n","            inputs = inputs.cuda()\n","            labels = labels.cuda()\n","\n","            # forward + backward + optimize\n","            outputs = net(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            # print statistics\n","            running_loss += loss.item()\n","            train_loss += loss.item()\n","            if i % 2000 == 1999:    # print every 2000 mini-batches\n","                print('[%d, %5d] loss: %.3f' %\n","                      (epoch + 1, i + 1, running_loss / 2000))\n","                running_loss = 0.0\n","                \n","\n","        # ==== Validation ====== #\n","        net.eval()\n","        # optimizer.zero_grad() [21.01.05 코드 클린업] 아래 torch.no_grad()가 호출되고 Validation 과정에서는 Optimizer를 사용하지 않으므로 굳이 호출될 필요가 없습니다. \n","        \n","        correct = 0\n","        total = 0\n","        val_loss = 0 \n","        with torch.no_grad():\n","            for data in valloader:\n","                images, labels = data\n","                images = images.view(-1, 3072)\n","                \n","                images = images.cuda()\n","                labels = labels.cuda()\n","                \n","                outputs = net(images)\n","\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item()\n","\n","                _, predicted = torch.max(outputs.data, 1)\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","\n","            val_loss = val_loss / len(valloader)\n","            val_acc = 100 * correct / total\n","            \n","        print('Epoch {}, Train Loss: {}, Val Loss: {}, Val Acc: {}'.format(epoch, train_loss, val_loss, val_acc ))\n","\n","\n","    # ===== Evaluation ===== #\n","    net.eval()\n","    # optimizer.zero_grad() [21.01.05 코드 클린업] 아래 torch.no_grad()가 호출되고 Evaluation 과정에서는 Optimizer를 사용하지 않으므로 굳이 호출될 필요가 없습니다. \n","    \n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for data in testloader:\n","            images, labels = data\n","            images = images.view(-1, 3072)\n","            images = images.cuda()\n","            labels = labels.cuda()\n","\n","            outputs = net(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","        test_acc = 100 * correct / total\n","            \n","    return train_loss, val_loss, val_acc, test_acc\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"omgExzmQgU1J"},"source":["## Experiment"]},{"cell_type":"code","metadata":{"id":"DRoOy_B3Wu7B"},"source":["seed = 123\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","\n","parser = argparse.ArgumentParser()\n","args = parser.parse_args(\"\")\n","\n","\n","args.n_layer = 5\n","args.in_dim = 3072\n","args.out_dim = 10\n","args.hid_dim = 100\n","args.act = 'relu'\n","\n","args.lr = 0.001\n","args.mm = 0.9\n","args.epoch = 2\n","\n","\n","list_var1 = [4, 5, 6]\n","list_var2 = [50, 100, 150]\n","\n","for var1 in list_var1:\n","    for var2 in list_var2:\n","        args.n_layer = var1\n","        args.hid_dim = var2\n","        result = experiment(args)\n","        print(result)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XVU233BqNKV6"},"source":["## 실험 결과 JSON 으로 저장하기"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b7vXr6mJNOqP","executionInfo":{"status":"ok","timestamp":1629356931107,"user_tz":-540,"elapsed":312,"user":{"displayName":"우태진","photoUrl":"","userId":"04263177752150657284"}},"outputId":"e0851a9f-1fa4-4108-98a4-5742b28ebafd"},"source":["# JSON 튜토리얼\n","\n","import json\n","\n","a = {'value1': 5, 'value2': 10, 'seq': [1, 2, 3, 4, 5]}\n","\n","filename = 'test.json'\n","with open(filename, 'w') as f: # with문은 명령이 끝나면 파일을 자동으로 닫아줌\n","    json.dump(a, f)\n","\n","with open(filename, 'r') as f:\n","    result = json.load(f)\n","    print(result)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'value1': 5, 'value2': 10, 'seq': [1, 2, 3, 4, 5]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jG2WXgGIQ4SU","executionInfo":{"status":"ok","timestamp":1629357624063,"user_tz":-540,"elapsed":309,"user":{"displayName":"우태진","photoUrl":"","userId":"04263177752150657284"}},"outputId":"30e55597-cda7-4069-b843-fece83972b1f"},"source":["# 해시 함수로 JSON 파일명 자동으로 따기 (파일명 선정 편의성 목적)\n","\n","import hashlib\n","\n","setting = {'value1': 5, 'value2': 10, 'seq': [1, 2, 3, 4, 5], 'exp_name': 'exp1'}\n","\n","exp_name = setting['exp_name']\n","hash_key = hashlib.sha1(str(setting).encode()).hexdigest()\n","filename = f'{exp_name}-{hash_key}.json'\n","print(filename)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["exp1-4766aa48bbb828e5bc6209c7f498a570f55da00e.json\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hMCU_Z_WRKei"},"source":["# 해시 함수로 JSON 파일에 실험 결과 저장하기\n","\n","import json\n","import hashlib\n","from os import listdir\n","from os.path import isfile\n","from os.path import isfile\n","\n","!mkdir results # results 라는 디렉토리 생성\n","!ls # 현재 위치에 있는 폴더, 파일 전부 보기\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9ppAeIN5XoRL"},"source":["## Implementing CNN with Pytorch"]},{"cell_type":"code","metadata":{"id":"AaYEsv03XtB0"},"source":["!mkdir results\n","\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import argparse\n","import numpy as np\n","import time\n","from copy import deepcopy # Add Deepcopy for args\n","import seaborn as sns \n","import matplotlib.pyplot as plt\n","\n","transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","trainset, valset = torch.utils.data.random_split(trainset, [40000, 10000])\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","partition = {'train': trainset, 'val':valset, 'test':testset}\n","\n","class MLP(nn.Module):\n","    def __init__(self, in_dim, out_dim, hid_dim, n_layer, act, dropout, use_bn, use_xavier):\n","        super(MLP, self).__init__()\n","        self.in_dim = in_dim\n","        self.out_dim = out_dim\n","        self.hid_dim = hid_dim\n","        self.n_layer = n_layer\n","        self.act = act\n","        self.dropout = dropout\n","        self.use_bn = use_bn\n","        self.use_xavier = use_xavier\n","        \n","        # ====== Create Linear Layers ====== #\n","        self.fc1 = nn.Linear(self.in_dim, self.hid_dim)\n","        \n","        self.linears = nn.ModuleList()\n","        self.bns = nn.ModuleList()\n","        for i in range(self.n_layer-1):\n","            self.linears.append(nn.Linear(self.hid_dim, self.hid_dim))\n","            if self.use_bn:\n","                self.bns.append(nn.BatchNorm1d(self.hid_dim))\n","                \n","        self.fc2 = nn.Linear(self.hid_dim, self.out_dim)\n","        \n","        # ====== Create Activation Function ====== #\n","        if self.act == 'relu':\n","            self.act = nn.ReLU()\n","        elif self.act == 'tanh':\n","            self.act == nn.Tanh()\n","        elif self.act == 'sigmoid':\n","            self.act = nn.Sigmoid()\n","        else:\n","            raise ValueError('no valid activation function selected!')\n","        \n","        # ====== Create Regularization Layer ======= #\n","        self.dropout = nn.Dropout(self.dropout)\n","        if self.use_xavier:\n","            self.xavier_init()\n","          \n","    def forward(self, x):\n","        x = self.act(self.fc1(x))\n","        for i in range(len(self.linears)):\n","            x = self.act(self.linears[i](x))\n","            if self.use_bn:\n","                x = self.bns[i](x)\n","            x = self.dropout(x)\n","        x = self.fc2(x)\n","        return x\n","    \n","    def xavier_init(self):\n","        for linear in self.linears:\n","            nn.init.xavier_normal_(linear.weight)\n","            linear.bias.data.fill_(0.01)\n","\n","cfg = {\n","    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n","    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n","}\n","\n","class CNN(nn.Module):\n","    \n","    def __init__(self, model_code, in_channels, out_dim, act, use_bn):\n","        super(CNN, self).__init__()\n","        \n","        if act == 'relu':\n","            self.act = nn.ReLU()\n","        elif act == 'sigmoid':\n","            self.act = nn.Sigmoid()\n","        elif act == 'tanh':\n","            self.act = nn.TanH()\n","        else:\n","            raise ValueError(\"Not a valid activation function code\")\n","        \n","        self.layers = self._make_layers(model_code, in_channels, use_bn)\n","        self.classifer = nn.Sequential(nn.Linear(512, 256),\n","                                       self.act,\n","                                       nn.Linear(256, out_dim))\n","        \n","    def forward(self, x):\n","        x = self.layers(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.classifer(x)\n","        return x\n","        \n","    def _make_layers(self, model_code, in_channels, use_bn):\n","        layers = []\n","        for x in cfg[model_code]:\n","            if x == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                layers += [nn.Conv2d(in_channels=in_channels,\n","                                     out_channels=x,\n","                                     kernel_size=3,\n","                                     stride=1,\n","                                     padding=1)]\n","                if use_bn:\n","                    layers += [nn.BatchNorm2d(x)]\n","                layers += [self.act]\n","                in_channels = x\n","        return nn.Sequential(*layers)\n","\n","class CNN1(nn.Module):\n","    \n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        \n","        self.conv1 = nn.Conv2d(in_channels = 3,\n","                               out_channels = 64,\n","                               kernel_size = 3,\n","                               stride = 1,\n","                               padding = 1)\n","        self.conv2 = nn.Conv2d(in_channels = 64,\n","                               out_channels = 256,\n","                               kernel_size = 5,\n","                               stride = 1,\n","                               padding = 2)\n","        self.act = nn.ReLU()\n","        self.maxpool1 = nn.MaxPool2d(kernel_size = 2,\n","                                     stride = 2)\n","        self.fc = nn.Linear(65536, 10)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.act(x)\n","        x = self.conv2(x)\n","        x = self.act(x)\n","        x = self.maxpool1(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc(x)\n","        return x\n","\n","def dimension_check():\n","    net = CNN('VGG11', 3)\n","    x = torch.randn(2, 3, 32, 32)\n","    y = net(x)\n","    print(y.size())\n","\n","def train(net, partition, optimizer, criterion, args):\n","    trainloader = torch.utils.data.DataLoader(partition['train'], \n","                                              batch_size=args.train_batch_size, \n","                                              shuffle=True, num_workers=2)\n","    net.train()\n","\n","    correct = 0\n","    total = 0\n","    train_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        optimizer.zero_grad() # [21.01.05 오류 수정] 매 Epoch 마다 .zero_grad()가 실행되는 것을 매 iteration 마다 실행되도록 수정했습니다. \n","\n","        # get the inputs\n","        inputs, labels = data\n","        inputs = inputs.cuda()\n","        labels = labels.cuda()\n","        outputs = net(inputs)\n","\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    train_loss = train_loss / len(trainloader)\n","    train_acc = 100 * correct / total\n","    return net, train_loss, train_acc\n","\n","def validate(net, partition, criterion, args):\n","    valloader = torch.utils.data.DataLoader(partition['val'], \n","                                            batch_size=args.test_batch_size, \n","                                            shuffle=False, num_workers=2)\n","    net.eval()\n","\n","    correct = 0\n","    total = 0\n","    val_loss = 0 \n","    with torch.no_grad():\n","        for data in valloader:\n","            images, labels = data\n","            images = images.cuda()\n","            labels = labels.cuda()\n","            outputs = net(images)\n","\n","            loss = criterion(outputs, labels)\n","            \n","            val_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","        val_loss = val_loss / len(valloader)\n","        val_acc = 100 * correct / total\n","    return val_loss, val_acc\n","\n","def test(net, partition, args):\n","    testloader = torch.utils.data.DataLoader(partition['test'], \n","                                             batch_size=args.test_batch_size, \n","                                             shuffle=False, num_workers=2)\n","    net.eval()\n","    \n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for data in testloader:\n","            images, labels = data\n","            images = images.cuda()\n","            labels = labels.cuda()\n","\n","            outputs = net(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","        test_acc = 100 * correct / total\n","    return test_acc\n","\n","def experiment(partition, args):\n","  \n","    net = CNN(model_code = args.model_code,\n","              in_channels = args.in_channels,\n","              out_dim = args.out_dim,\n","              act = args.act,\n","              use_bn = args.use_bn)\n","    net.cuda()\n","\n","    criterion = nn.CrossEntropyLoss()\n","    if args.optim == 'SGD':\n","        optimizer = optim.SGD(net.parameters(), lr=args.lr, weight_decay=args.l2)\n","    elif args.optim == 'RMSprop':\n","        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.l2)\n","    elif args.optim == 'Adam':\n","        optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2)\n","    else:\n","        raise ValueError('In-valid optimizer choice')\n","    \n","    train_losses = []\n","    val_losses = []\n","    train_accs = []\n","    val_accs = []\n","        \n","    for epoch in range(args.epoch):  # loop over the dataset multiple times\n","        ts = time.time()\n","        net, train_loss, train_acc = train(net, partition, optimizer, criterion, args)\n","        val_loss, val_acc = validate(net, partition, criterion, args)\n","        te = time.time()\n","        \n","        train_losses.append(train_loss)\n","        val_losses.append(val_loss)\n","        train_accs.append(train_acc)\n","        val_accs.append(val_acc)\n","        \n","        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n","        \n","    test_acc = test(net, partition, args)    \n","    \n","    result = {}\n","    result['train_losses'] = train_losses\n","    result['val_losses'] = val_losses\n","    result['train_accs'] = train_accs\n","    result['val_accs'] = val_accs\n","    result['train_acc'] = train_acc\n","    result['val_acc'] = val_acc\n","    result['test_acc'] = test_acc\n","    return vars(args), result\n","\n","import hashlib\n","import json\n","from os import listdir\n","from os.path import isfile, join\n","import pandas as pd\n","\n","def save_exp_result(setting, result):\n","    exp_name = setting['exp_name']\n","    del setting['epoch']\n","    del setting['test_batch_size']\n","\n","    hash_key = hashlib.sha1(str(setting).encode()).hexdigest()[:6]\n","    filename = './results/{}-{}.json'.format(exp_name, hash_key)\n","    result.update(setting)\n","    with open(filename, 'w') as f:\n","        json.dump(result, f)\n","\n","    \n","def load_exp_result(exp_name):\n","    dir_path = './results'\n","    filenames = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) if '.json' in f]\n","    list_result = []\n","    for filename in filenames:\n","        if exp_name in filename:\n","            with open(join(dir_path, filename), 'r') as infile:\n","                results = json.load(infile)\n","                list_result.append(results)\n","    df = pd.DataFrame(list_result) # .drop(columns=[])\n","    return df"],"execution_count":null,"outputs":[]}]}